{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's compare accuracy of some models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from helpers.highlevel_sceneloader import HighLevelSceneLoader\n",
    "from predictors.dataset_creator import TFDataSet\n",
    "import tensorflow as tf\n",
    "from predictors.dl_trainer import DLTrainer \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = os.getcwd()\n",
    "\n",
    "rel_p_img_b = 'helpers/analysed_vars_storage/img_bounds.xml'\n",
    "rel_p_dests = 'helpers/analysed_vars_storage/destination_locations.xml'\n",
    "p_img_bounds = os.path.join(ROOT, rel_p_img_b)\n",
    "p_dest_locs = os.path.join(ROOT, rel_p_dests)\n",
    "\n",
    "#TODO: older version of OpenTraj needed: \"git checkout d249ba6951dd0f54b532fbe2ca6edc46b0d7093f\"\n",
    "opentraj_root = os.path.join(ROOT, 'OpenTraj')\n",
    "root_datasets = os.path.join(ROOT, 'data/path_data')\n",
    "sys.path.append(opentraj_root) # add package to pythonpath\n",
    "\n",
    "scene_data = HighLevelSceneLoader(p_img_bounds, p_dest_locs)\n",
    "scene_data.load_ind(root_datasets, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 50\n",
    "PATIENCE = 10\n",
    "LSTM_LAYER_SIZE = 64\n",
    "DENSE_LAYER_SIZE = 128\n",
    "NUM_LSTM_LAYERS = 2\n",
    "NUM_DENSE_LAYERS = 2\n",
    "DS_LOCATION = \"data/pickle/ds_creation_d/calc_before_all_f.pickle\"\n",
    "\n",
    "NOISE_STD = .15\n",
    "N_REPEATS = 7\n",
    "SEQ_IN_LENGTH = 5\n",
    "SEQ_OUT_LENGTH = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: no extra features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_features_dict = {}\n",
    "\n",
    "# Load data in order to not need to do calculations again\n",
    "with open(DS_LOCATION, 'rb') as handle:\n",
    "    my_ds_creation_dict = pickle.load(handle)\n",
    "\n",
    "my_ds = TFDataSet.init_as_fixed_length(scene_data.traj_dataframe, scale_list=[\"pos_x\", \"pos_y\"], seq_in_length=SEQ_IN_LENGTH, label_length=SEQ_OUT_LENGTH, seq_stride=1,\n",
    "extra_features_dict=extra_features_dict, ds_creation_dict=my_ds_creation_dict, noise_std=NOISE_STD, n_repeats = N_REPEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(my_ds.tf_ds_dict[\"train\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Total params: 61,826\n",
      "Trainable params: 61,826\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bram/anaconda3/envs/thesis_basic/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:588: UserWarning: Input dict contained keys ['labels', 'all_points', 'n_destinations', 'n_points'] which did not match any model input. They will be ignored by the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "924/924 [==============================] - 12s 10ms/step - loss: 0.0459 - mean_absolute_error: 0.1696 - val_loss: 0.0170 - val_mean_absolute_error: 0.1020\n",
      "Epoch 2/50\n",
      "924/924 [==============================] - 9s 9ms/step - loss: 0.0066 - mean_absolute_error: 0.0605 - val_loss: 0.0052 - val_mean_absolute_error: 0.0523\n",
      "Epoch 3/50\n",
      "924/924 [==============================] - 8s 8ms/step - loss: 0.0034 - mean_absolute_error: 0.0435 - val_loss: 0.0032 - val_mean_absolute_error: 0.0410\n",
      "Epoch 4/50\n",
      "924/924 [==============================] - 8s 8ms/step - loss: 0.0025 - mean_absolute_error: 0.0381 - val_loss: 0.0036 - val_mean_absolute_error: 0.0487\n",
      "Epoch 5/50\n",
      "924/924 [==============================] - 8s 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0374 - val_loss: 0.0021 - val_mean_absolute_error: 0.0361\n",
      "Epoch 6/50\n",
      "924/924 [==============================] - 8s 8ms/step - loss: 0.0025 - mean_absolute_error: 0.0381 - val_loss: 0.0026 - val_mean_absolute_error: 0.0408\n",
      "Epoch 7/50\n",
      "924/924 [==============================] - 8s 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0386 - val_loss: 0.0030 - val_mean_absolute_error: 0.0444\n",
      "Epoch 8/50\n",
      "924/924 [==============================] - 8s 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0374 - val_loss: 0.0020 - val_mean_absolute_error: 0.0349\n",
      "Epoch 9/50\n",
      "924/924 [==============================] - 8s 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0369 - val_loss: 0.0018 - val_mean_absolute_error: 0.0323\n",
      "Epoch 10/50\n",
      "924/924 [==============================] - 7s 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0366 - val_loss: 0.0032 - val_mean_absolute_error: 0.0446\n",
      "Epoch 11/50\n",
      "924/924 [==============================] - 8s 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0372 - val_loss: 0.0037 - val_mean_absolute_error: 0.0461\n",
      "Epoch 12/50\n",
      "924/924 [==============================] - 8s 8ms/step - loss: 0.0022 - mean_absolute_error: 0.0361 - val_loss: 0.0023 - val_mean_absolute_error: 0.0376\n",
      "Epoch 13/50\n",
      "924/924 [==============================] - 8s 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0367 - val_loss: 0.0022 - val_mean_absolute_error: 0.0359\n",
      "Epoch 14/50\n",
      "924/924 [==============================] - 8s 8ms/step - loss: 0.0021 - mean_absolute_error: 0.0353 - val_loss: 0.0020 - val_mean_absolute_error: 0.0351\n",
      "Epoch 15/50\n",
      "924/924 [==============================] - 8s 8ms/step - loss: 0.0019 - mean_absolute_error: 0.0332 - val_loss: 0.0025 - val_mean_absolute_error: 0.0382\n",
      "Epoch 16/50\n",
      "924/924 [==============================] - 8s 8ms/step - loss: 0.0021 - mean_absolute_error: 0.0349 - val_loss: 0.0018 - val_mean_absolute_error: 0.0324\n",
      "Epoch 17/50\n",
      "924/924 [==============================] - 8s 8ms/step - loss: 0.0019 - mean_absolute_error: 0.0337 - val_loss: 0.0031 - val_mean_absolute_error: 0.0444\n",
      "Epoch 18/50\n",
      "924/924 [==============================] - 8s 9ms/step - loss: 0.0021 - mean_absolute_error: 0.0351 - val_loss: 0.0036 - val_mean_absolute_error: 0.0472\n",
      "Epoch 19/50\n",
      "924/924 [==============================] - 10s 11ms/step - loss: 0.0020 - mean_absolute_error: 0.0341 - val_loss: 0.0028 - val_mean_absolute_error: 0.0424\n"
     ]
    }
   ],
   "source": [
    "my_trainer_basic = DLTrainer(max_epochs=MAX_EPOCHS, patience=PATIENCE)\n",
    "network = my_trainer_basic.LSTM_one_shot_predictor_named_i(my_ds, LSTM_LAYER_SIZE, DENSE_LAYER_SIZE, \n",
    "NUM_LSTM_LAYERS, NUM_DENSE_LAYERS, extra_features=[])\n",
    "\n",
    "save_path = \"data/model_weights/m_no_extra_f.h5\"\n",
    "\n",
    "try:\n",
    "    my_trainer_basic.load_weights(save_path)\n",
    "except:\n",
    "    my_trainer_basic.compile_and_fit(my_ds, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Added Feature: All Destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trainer_all_dests = DLTrainer(max_epochs=MAX_EPOCHS, patience=PATIENCE)\n",
    "network = my_trainer_all_dests.LSTM_one_shot_predictor_named_i(my_ds, LSTM_LAYER_SIZE, DENSE_LAYER_SIZE, \n",
    "NUM_LSTM_LAYERS, NUM_DENSE_LAYERS, extra_features=[\"all_destinations\"])\n",
    "\n",
    "save_path = \"data/model_weights/m_all_dests.h5\"\n",
    "\n",
    "my_trainer_all_dests.load_weights(save_path)\n",
    "try:\n",
    "    my_trainer_all_dests.load_weights(save_path)\n",
    "except:\n",
    "    my_trainer_all_dests.compile_and_fit(my_ds, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Added feature: All points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trainer_all_points = DLTrainer(max_epochs=MAX_EPOCHS, patience=PATIENCE)\n",
    "network = my_trainer_all_points.LSTM_one_shot_predictor_named_i(my_ds, LSTM_LAYER_SIZE, DENSE_LAYER_SIZE, \n",
    "NUM_LSTM_LAYERS, NUM_DENSE_LAYERS, extra_features=[\"all_points\"])\n",
    "\n",
    "save_path = \"data/model_weights/m_all_points.h5\"\n",
    "\n",
    "my_trainer_all_points.load_weights(save_path)\n",
    "try:\n",
    "    my_trainer_all_points.load_weights(save_path)\n",
    "except:\n",
    "    my_trainer_all_points.compile_and_fit(my_ds, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('thesis_basic': conda)",
   "name": "python385jvsc74a57bd0670cfe7939475ebb968c924b13a14a1c26d345d90444bab171f80d257de98d85"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "670cfe7939475ebb968c924b13a14a1c26d345d90444bab171f80d257de98d85"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}